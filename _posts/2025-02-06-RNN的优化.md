---
title: RNN的优化
date: 2025-02-06 23:51:00 +0800
categories: [Study, LLM]
tags: [study]
---
# RNN的优化
由于普通RNN的效果不是很好，除了LSTM这种改进模型以外还有没有其他的方法呢？
## 多层RNN
多层RNN很简单，通过搭建多个RNN层来提升模型的能力，可以看作把一个输入进过了多层RNN参数处理以后的结果，类似于图中的结构


![alt text](/assets/2025-02-06-S1.png)


可以看出，中间层的RNN需要传输给下一层相关的信息，所以在代码上：
```
from keras.models import Sequential
from keras.layers import SimpleRNN,Embedding,Dense

vocabular_size = 1000
Embedding_size = 32
word_num = 20
state_dim = 32

model = Sequential()
model.add(Embedding(vocabular_size,Embedding_size,input_length=word_num))
model.add(SimpleRNN(state_dim,return_sequences=True))
model.add(SimpleRNN(state_dim,return_sequences=True))
model.add(SimpleRNN(state_dim,return_sequences=False))
model.add(Dense(1,activation='sigmoid'))
model.summary()
```
可以注意到上面前面几层的返回都是包含全部的信息的
## 双向RNN
大家平时开玩笑说文字阅读不影响顺序，事实也是如此，在文字的表述上大部分时候不会因为正着读反着读而产生差别。
双向RNN通过一次从前往后的RNN训练以及一次从后往前的训练缓解了RNN的遗忘问题，以前忘记的现在立刻记一下


![alt text](/assets/2025-02-06-S2.png)


在代码上可以引入Bidirectional这个层，它可以把一个RNN变成双向的
```
from keras.models import Sequential
from keras.layers import SimpleRNN,Embedding,Dense,Bidirectional

vocabular_size = 1000
Embedding_size = 32
word_num = 20
state_dim = 32
model.add(Embedding(vocabular_size,Embedding_size,input_length=word_num))
model.add(Bidirectional(SimpleRNN(state_dim,return_sequences=False)))
model.add(Dense(1,activation='sigmoid'))
model.summary()
```
## 预训练
预训练可以解决参数比较庞大但是训练样本少的情况
首先可以找到类似相近问题的相关数据集，搭建一个模型进行训练（不一定是RNN）训练完模型后保留Embedding层的权重，然后再移花接木把Embedding层给要训练的RNN来使用（只训练其他层，不训练Embedding层）