---
title: DeepSeek R1论文解读
date: 2025-02-01 23:57:00 +0800
categories: [Study, LLM]
tags: [study]
---

# DeepSeek R1论文解读
> **⚠️ 警告**  
> 本人水平有限，并不能保证内容的正确性

这次DeepSeek R1论文的主要三个点可以从论文的目录看出


![alt text](/assets/2025-02-01-S1.png)


主要是有两个重点：
**1、在底座模型上进行强化学习**
**2、蒸馏后的小模型性能优秀**

首先需要明白LLM的主要的一些基础概念以及强化学习的基础概念
### 语言模型
一个语言模型可以看作是一个预测模型，它可以根据历史数据预测下一个词或者句子，也就是所谓的根据上下文


![alt text](/assets/2025-02-01-S2.png)


### 强化学习的核心流程
#### 目标：
强化学习的目标是通过学习一个策略（Policy），使得智能体（Agent）在与环境（Environment）的交互中，最大化长期累积奖励（Expected Return）
#### 基本要素：


Agent：决策主体，通过策略选择动作（Action）。
Environment：Agent 交互的外部世界，根据当前状态（State）和动作返回下一个状态和奖励（Reward）。
State (s)：环境的当前状态（Agent 的观察结果）。
Action (a)：Agent 在某个状态下采取的动作。
Reward (r)：环境对 Agent 动作的反馈信号。
Policy (π)：Agent 的决策规则（从状态到动作的映射）。


#### 标准交互流程
初始化：
环境处于初始状态 s0​。
循环交互：
步骤 1：Agent 根据当前状态 st 和策略 π(a∣st) 选择动作 at​。
步骤 2：环境接收动作 at​，返回下一状态 st+1​ 和奖励 rt+1​。
步骤 3：Agent 根据 st,at,rt+1,st+1 更新策略（如 Q-learning、策略梯度）。
步骤 4：重复直到任务终止或策略收敛。

在训练LLM时，我们的policy决策环节一般就是LLM本身，因为LLM本身就是一个预测模型，而在训练的时候我们希望我们可以获得较好的效果，所以会做对齐操作，使得LLM输出的内容更加准确。所以一般会列出一系列问题，LLM生产一系列的回答，请专门的人标注我们更加希望LLM回答出来的结果，这作为奖励模型的一环，最后训练出一个模型。这么做可以让LLM输出更多的是标注者选择的答案


### 在底座模型上进行强化学习


![alt text](/assets/2025-02-01-S3.png)

通常，LLM在进行训练的时候会进行监督学习，但是这会消耗大量的内存时间以及人力成本，所以DeepSeek R1提出了一种直接在底座模型上进行强化学习的方法。DeepSeek希望通过强化学习增强模型的推理能力，模型可以通过奖励机制自主发展，自主学习哪些内容应该是人类更加喜欢会进行标注的（不需要人工标注）。
DeepSeek R1使用的强化学习策略与以往常用的PPO算法有所不同，使用了一种叫GRPO的算法策略


![alt text](/assets/2025-02-01-S4.png)


在以往的强化学习中，我们一般需要构建一个偏好的数据集来训练价值函数，使得模型更加贴近人类喜好，但正如上面所讲，会增加成本以及耗费大量时间，PPO一般需要训练一个相当规模的Value Model来进行评判。


![alt text](/assets/2025-02-01-S5.png)


上面是强化学习需要解决的优化问题，最大化长期累积奖励函数J𝐺𝑅𝑃𝑂(𝜃)
模型输出：𝑜𝑖
训练策略：𝜋
当前策略：𝜋𝜃(𝑜𝑖|𝑞)
之前策略：𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖|𝑞)
优势函数：𝐴𝑖，告诉我们在当前输出的词相较于其他的可能性有多大优势
KL散度：D𝐾𝐿(𝜋𝜃||𝜋𝑟𝑒𝑓)，衡量分布之间的距离差异，作为分布差异的软性约束，保持策略更新的平滑性
裁剪函数：clip(rt(θ),1−ϵ,1+ϵ)，约束项，将大小限制在(1−ϵ,1+ϵ)之间
总体来说，函数的设计思路是求解一个优化问题，即最大化长期累积奖励函数J𝐺𝑅𝑃𝑂(𝜃)，为了避免整体的策略更新过大，使用了KL散度作为约束项，同时使用了裁剪函数来限制策略更新的幅度，对比起PPO算法，GRPO算法的优势在于它不需要训练一个Value Model，而是直接使用底座模型的推理结果来进行训练，节省了时间以及成本，更进一步说，它用优势函数𝐴𝑖来替代了此前PPO的Value Model。min和-𝛽D𝐾𝐿(𝜋𝜃||𝜋𝑟𝑒𝑓)：限制策略更新的幅度，防止因单次更新过大导致策略崩溃，我们更加希望他们通过多次迭代从而来使得累计得分最高而不是一步跨太大导致模型崩溃或者通过“作弊”的方式来仅仅使得得分最高但不是我们希望看到的输出内容。这种双重约束的优化框架体现了强化学习中经典的探索-利用平衡思想，在提升策略表现与保持学习稳定性之间建立了动态平衡机制。
πθ(oi|q)/π_old(oi|q)通常出现在强化学习中，尤其是在策略优化算法（如策略梯度方法）的上下文中。这个公式涉及到策略的更新和重要性采样，特别是在离线强化学习。在离线强化学习中，我们通常有一个固定的、预先收集的数据集 D，这些数据是由一个旧的策略π old生成的。由于我们不能直接与环境交互，因此需要利用这些数据来优化新的策略 πθ。重要性采样就是上述的公式，它衡量了新策略 πθ 相对于旧策略 π old 的优势，并根据这个优势来调整数据分布，以便于新策略的优化。
重要性采样：重要性采样是一种技术，用于估计一个分布下的期望值，而数据是从另一个分布中采样的。在离线 RL 中，我们使用重要性采样来调整旧策略生成的数据，使其适用于新策略的优化。
重要性采样权重定义为：
PPO的得分策略一般是标注者所标记的相关词会获得更高的得分，而不是的得分往往会很低，它往往会输出一个具体的数值化的奖励信号告诉模型哪些是更加讨喜的而哪些是错误的。
那既然没有人工标注的数据，模型怎么知道哪些是正确的，哪些是错误的呢？原来GRPO的得分策略有所不同，GRPO使用基于规则的奖励机制。关于正确性的判断，它不关注你的结果怎么得出来的，如果你输出的代码可以通过编译器，正常运行，得出结果，那就是好代码；如果你的数学得出的结果可以在设置的特定的程序里通过，那也是好结果。关于格式的问题，如果加上思考的标签那会得分更高。


![alt text](/assets/2025-02-01-S6.png)


强化学习的精妙之处在于它当模型执行了一系列操作以后才知道得分的多少，但得分的多少又可以反馈到下一次每一步上使得下一次可以更加高可能性的去走得分更高的道路。
两者对比前者更像是给出答案让人学习，后者是让人自己摸索哪种答案得分比较高，如同教小孩子走路一样，我不告诉你具体哪里怎么发力，你能走起来慢慢摸索自然而然的就能走了


### 蒸馏后的小模型性能优秀
蒸馏模型是指通过大模型来训练模型的一种方式，大模型输出的内容并不像人工输出的内容标签一样是个确切的值而是更加符合大语言模型的概率的软标签。核心思想是教师模型通过其预测结果（如概率分布或推理过程）向学生模型传授知识，而学生模型通过学习这些结果逐步提升自己的性能。


![alt text](/assets/20250131_S4.png)


![alt text](/assets/2025-02-01-S7.png)


可以看出，通过DeepSeek R1的蒸馏，小模型的性能优秀，在测试上甚至完成了“越级单杀”，但是也能看出小模型是需要依赖大模型的，如果大模型的能力没有很大提升，蒸馏的小模型也会很难有很大的提升。