---
title: Deepseek蒸馏模型的本地部署
date: 2025-01-31 23:56:00 +0800
categories: [Study, LLM]
tags: [study]
---

## Deepseek蒸馏模型的本地部署
首先需要现在网上所谓很多的DeepSeek相关模型的本地部署都是扯淡的，以pilipili举例：
![alt text](/assets/20250131_S1.png)
可以看到有很多说本地部署模型的，这些其实有些混淆概念了，真正的Deepseek-R1模型是这下面这个
![alt text](/assets/20250131_S2.png)
部署它需要极大的内存，总共参数达到了671B，也只有上述两个模型的版本，它部署的成本最低应该为6k美金左右；其他所谓的小模型是属于Deepseek的蒸馏模型,是下面图片描述的相关内容
![alt text](/assets/20250131_S3.png)
所谓的蒸馏模型是只通过大模型来训练模型的一种方式，大模型输出的内容并不像人工输出的内容标签一样是个确切的值而是更加符合大语言模型的概率的软标签。核心思想是教师模型通过其预测结果（如概率分布或推理过程）向学生模型传授知识，而学生模型通过学习这些结果逐步提升自己的性能。
![alt text](/assets/20250131_S4.png)
现在网络上几乎所有的教程是教你如何部署这些由DeepSeek调教出来的小模型例如阿里的千问，他们描述的属于是有些标题党了，真正的部署教程是部署Deepseek蒸馏模型，接下来是关于部署的教程。


## 在Windows系统以及Ubuntu上部署Deepseek蒸馏模型
首先我们到ollama的官网上下载ollama
https://ollama.com/
下载后它会默认安装到C盘 打开cmd输入ollama查看安装是否成功
![alt text](/assets/20250131_S5.png)
接下来在ollama官网上搜索相关命令去下载运行相关的模型
![alt text](/assets/20250131_S6.png)
以我电脑的14B举例 输入的命令应该为
```
ollama run deepseek-r1:14b
```
如果下载了多个模型，可以通过来显示全部模型
```
ollama list
```
从右侧复制相关的下载运行命令来下载或者运行模型，由于我已经下载过了所以这里就不展示整个的下载过程了，由于是在本地运行所以第一首先要选择适合自己电脑的模型，可以查看自己电脑的任务管理器里面性能的内存大小来进行推断自己最高可以跑多大的模型


下载完以后就可以开始运行了
![alt text](/assets/20250131_S7.png)
直接在命令行输入相关的文字内容就可以开始对话了，可以观察到输出有思考的过程还有给出的结果，这里让他写一首诗测试一下
![alt text](/assets/20250131_S8.png)
同理，mac系统上部署相关系统也是类似的套路，这里就不再赘述了。
## 在安卓系统上部署Deepseek蒸馏模型
与上述类似，可以通过下载Terminal一类的软件例如termux来搭建类似的Ubuntu环境，然后按照上述的步骤下载运行相关的模型。
我不用安卓系统所以也没办法进行特别详细的示范，但是确实是可行的
这里给出一个参考链接：https://www.coolapk.com/feed/62404975?shareKey=NWUwMDBlM2E0MzM4Njc5OWZkMzg~&shareUid=805613&shareFrom=com.coolapk.market_15.0.2


## 在IOS上部署Deepseek蒸馏模型
### 第一种方案 Linux虚拟机软件 不推荐
是使用相关的Linux虚拟机软件来实现相关的部署，我使用过ish来进行部署但是由于ish模拟的Linux系统是32位的而ollama是只支持64位的；我并没有尝试用A—Shell来部署，但是理论上如果是64位的话是可行的。
### 第二种方案 MLC LLM项目 较为复杂
第二种方法是通过MLC相关的项目来进行部署，这需要Mac系统的电脑来访问你的手机内存来对系统文件进行修改，简单来说就是把模型文件移动到MLC chat软件可以识别到的目录下，然后启用软件。但这个方案需要用到X code来进行相关的开发，并且需要一定的编程基础而且要求有台Mac（我没有；；）
这里贴出相关的连接
项目的总目录：https://github.com/mlc-ai/mlc-llm
项目ios端部署官方的教程步骤：https://llm.mlc.ai/docs/deploy/ios.html
### 第三种方案 使用相关软件进行搭建 简单无脑
ios端其实有很多搭建LLM相关的软件，这里我推荐PocketPal，他有直接从IOS文件管理软件导入的选项也有从小黄脸下载的选项，是个友好的选择
（其他的软件理论上也能通过下载模型后导入实现）
![alt text](/assets/20250131_S9.png)
下载好以后就可以load相关模型进行运行了
![alt text](/assets/20250131_S10.png)
查看相关效果
![alt text](/assets/20250131_S11.png)

## 总结
通过以上平台的部署方法，你就可以在自己的设备上运行相关的大语言模型
但是我只能说效果肯定是差强人意的，因为模型的大小影响，内存的大小影响，硬件的性能影响，问答的效果绝对是不如你直接上网使用大语言模型的。
我看到网上有人说本地部署是伪需求，这绝不是伪需求，它具有本地不联网的绝对优势就是安全，最理想的情况下是涉及隐私的相关信息就在本地处理或者像Apple intelligent一样在本地运行一些进过训练调试的小规模特定领域的相关模型进行运行，满足隐私相关的保护