---
title: NLP的数据处理 文本处理以及词嵌套
date: 2025-02-02 00:23:03 +0800
categories: [Study, NLP]
tags: [study]
---
# 数据处理基础

> 参考王树森的RNN & Transformer 
> link：https://github.com/wangshusen/DeepLearning


### 数据特征大致分为三类：
**1：数值特征**：类似于价格等具体的数值
**2：二元特征**：能够表示成0/1的特征，如性别、是否收到过礼物等
**3：分类特征**：类似于表示这个东西属于某一类的特征


在分类特征中，因为具体的数值只表示特定的一个类别，所以一般采用one-hot编码，即独热编码的方式进行编码。独热编码就是将某个特征的某个值置为1，其他值置为0，如图例子：


![alt text](/assets/2025-02-02-S1.png)


在这种例子中，需要把0给单独保留出来来应付缺失值
### WHY：为什么需要每位单独一个特征为1其他置0呢？
因为分类特征不可以进行数值计算，如果每个数字分配一个类别，那么就会导致特征之间可以计算；同时由于每一位都是单独的其他置0的特性，也更容易进行位运算
同时呢，保留也有好处，也有可能在特别的例子里有特别的情况，就好比视频分类或者tag，既有可能是科技，也有可能是计算机技术，如果使用独热编码就可以很好的表示而不产生歧义
在机器学习中，不能用标量表示分类特征，他们之间不可以进行数值运算


对于文本来说，假设有10个词，那就是有10个分类特征（categorical feature），所以我们在进行NLP项目时首先会对这些文本进行处理


![alt text](/assets/2025-02-02-S2.png)


第一步需要进行Tokenization，即将文本（或者说是字符串）分割成词


![alt text](/assets/2025-02-02-S3.png)


第二步是利用哈希表来统计词频（python里面的字典）
统计词频后把词频转换成index，一般词频最大的index为1，把单词映射到数字；如果词频统计的哈希表过大可以只保留前面频率最高的，这样就达到了筛选低频词的效果
移去低频词可以去掉可能由拼写错误造成的错误以及使得独热编码的维度不会特别高从而减少计算量
转换成独热编码以后有多少个index就有多少个特征，独热编码就有多少维


# 文本处理
通常得到词频字典时，我们需要做一些操作来去除对于我们工作没有用的一些词，比如停用词、标点符号、数字等，但是里面的门道就是很讲究了，例如在处理英语文本时是否应该把所有字词都转成小写呢？对于一些词而言（e.h Apple vs. apple）在不同语境下有不同的意思。对于常用词和一些错别字 拼写错误又该怎么处理呢？具体情况不同的处理方式


有了字典进行了独热编码以后，字符串就转换成为了一个向量，但是问题是由于不同的信息长度不同，我们的向量长度是不一样的，这个时候没有办法成为一个矩阵进行计算，所以我们需要对文本进行padding，即使文本长度不一样，没有进行对齐。
一般的对齐方法分为两种：
1、裁剪：过长的部分舍弃
2、补齐：补齐到统一长度，一般用0填充

现在解决了对齐的问题，随之而来的又是另外一个问题：我们不希望我们的输入是一个几万维度的向量，所以我们需要进行降维
# 词嵌套


![alt text](/assets/2025-02-02-S4.png)


我们将得到的独热编码向量进行转置，PT是一个d*v的矩阵，其中d是词向量的维度，由用户自己决定；v是独热编码的维度；P转置每一列就是一个词向量，用于映射


![alt text](/assets/2025-02-02-S5.png)


可以看出，我们设定的词向量维度为二维，所以可以表示在二维平面上，正面词应该和负面词离得远一些，中心词应该在二者中间
```
from keras.layers import Embedding
fronm keras.models import Sequential

embedding_layer = 8#用户自己指定的词向量维度
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_layer, input_length=max_len))

#vocab_size是词频的长度，也就是独热编码的维度
#embedding_layer是词向量的维度
#input_length是文本的最大长度
#我们总共有vocab_size个词，每个词的维度是embedding_layer
#所以我们的参数有vocab_size*embedding_layer
```
# EXP：


![alt text](/assets/2025-02-02-S6.png)


Sequential模型的作用正是将神经网络层按顺序逐层堆叠，形成一个线性、无分支的拓扑结构。这里我们将模型embedding层按照上面的方法处理后使用Flatten层将词向量展开成一维向量，然后使用Dense层进行分类，使用的激活函数是sigmoid 输出0/1的概率


![alt text](/assets/2025-02-02-S7.png)


这里的优化函数是RMSprops
评价指标是acc，训练epochs轮。epochs是训练的轮数，会每次把所有的样本都训练一遍。
