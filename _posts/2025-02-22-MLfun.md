---
title: 深度学习和机器学习中的重要概念
date: 2025-02-22 17:00:00 +0800
categories: [Study, ML]
tags: [study]
---
>感觉自己跟个文盲一样，有些概念如今还是摸棱两可，故此写这一个名词性的总结
# 深度学习和机器学习中的重要概念

## 1. Batch（批次）
定义: Batch 是指一次迭代（iteration）中用于计算梯度并更新模型参数的样本集合。（大白话来讲就是一次迭代的样本）
Batch Size（批次大小）: 每次迭代使用的样本数量。例如，batch size = 32 表示每次迭代使用 32 个样本来计算梯度。
作用:
在训练过程中，模型并不是一次性使用整个数据集来计算梯度（这样计算开销太大，当然也不一定，也许有人喜欢呢），而是将数据分成多个 batch，每次处理一个 batch。通过 batch，可以平衡计算效率和内存使用。

>exp:假设数据集有 1000 个样本，batch size = 100，那么每次迭代会使用 100 个样本计算梯度，更新模型参数。
## 2. Epoch（轮次）
定义: Epoch 是指整个训练数据集被完整遍历一次的过程。
作用:
一个 epoch 表示模型已经看过了整个训练数据集一次,通常需要多个 epoch 才能让模型充分学习数据的特征。
注意，epoch注意的是整个训练集训练的次数
### 与 Batch 的关系
一个 epoch 包含多个 batch。具体来说，一个 epoch 的迭代次数 = 总样本数 / batch size。

>exp:假设数据集有 1000 个样本，batch size = 100，那么一个 epoch 会包含 10 次迭代（1000 / 100 = 10）。
## 3. Iteration（迭代）
定义: 一次迭代是指使用一个 batch 的数据来计算梯度并更新模型参数的过程。
与 Batch 的关系: 每次处理一个 batch 就是一次迭代。
>exp:如果数据集有 1000 个样本，batch size = 100，那么一个 epoch 包含 10 次迭代。
## 4.Optimizer（优化器）
定义: 优化器是用于更新模型参数的算法，基于梯度信息调整参数。

常见优化器:
SGD（随机梯度下降）
Momentum（动量法）
Adam（自适应矩估计）
RMSProp（均方根传播）

## 5.Hyperparameter（超参数）
定义: 超参数是模型训练前需要手动设置的参数，如学习率、batch size、网络层数等。
作用: 超参数的选择直接影响模型的性能和训练效率。

## 6.Loss（损失函数）
定义: 损失函数用于衡量模型预测值与真实值之间的差异。
作用: 损失函数是模型优化的目标，通过最小化损失函数来训练模型。
常见损失函数:
回归任务：均方误差（MSE）。
分类任务：交叉熵损失（Cross-Entropy Loss）。

## 7.Regularization（正则化）
定义: 正则化是通过在损失函数中添加额外项来限制模型复杂度，防止过拟合。

常见方法:
L1 正则化（Lasso）：添加参数的绝对值之和。
L2 正则化（Ridge）：添加参数的平方和。
Dropout：在训练过程中随机丢弃部分神经元。

## Learning Rate（学习率）
定义: 学习率是控制模型参数更新步长的超参数。它决定了每次迭代中参数根据梯度调整的幅度。
作用:
学习率过小：模型收敛速度慢，训练时间过长。
学习率过大：模型可能无法收敛，甚至发散。
调整方法: 学习率调度（Learning Rate Scheduling），如学习率衰减（Learning Rate Decay）、余弦退火（Cosine Annealing）等。