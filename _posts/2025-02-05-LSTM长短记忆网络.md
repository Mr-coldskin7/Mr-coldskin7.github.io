---
title: Simple RNN
date: 2025-02-05 23:34:00 +0800
categories: [Study, LLM]
tags: [study]
---
# LSTM
长短记忆神经网络与RNN的结构类似，都是循环神经网络


![alt text](/assets/2025-02-05-S1.png)


LSTM神经网络有一个传送带来传递过去的信息（长期记忆），来避免梯度消失，保留了短期记忆（之前一个的状态，因为它会忘记之前的状态，RNN上面体现的很明白）。
LSTM有很多门，首先先看看遗忘门：
## 遗忘门


![alt text](/assets/2025-02-05-S2.png)


输入a进入时首先会先经过sigmoid函数将输出固定在0到1之间，然后进行Elementwise multiplication，再与上一个时刻的cell state相乘，得到遗忘门的输出。


![alt text](/assets/2025-02-05-S3.png)


而具体的a输入是通过类似RNN的方式，把上个状态h和现在的输入x拼接以后乘以由训练集学习的权重Wf矩阵中，输出后的内容再进过sigmoid函数，得到遗忘门的输出。

![alt text](/assets/2025-02-05-S4.png)


这里我们已经能够大概清楚这里的输入代表什么了，这里的输入相当于记住长期记忆的百分比，如果小于等于0，它将会忘记之前的东西，第一层的遗忘门决定了要忘记什么。而传送门相当于原有的长期记忆的传递。
## 输入门


![alt text](/assets/2025-02-05-S5.png)


输入门的输入与遗忘门类似，其中一个输入input，决定了传送门的数值并且会更新，激活函数也是sigmoid，也是同上将上个状态和输入拼接以后乘以参数矩阵Wi，再经过sigmoid函数。
这里类比上面的遗忘门，我们可以明白通过左边的一块非常接近上层的遗忘门的思路，它类似与一种决定潜在记忆中哪些部分加入到新的记忆中，而右边的一块则是将短期记忆以及输入结合，形成了一种潜在长期记忆，总的来说就是左边的是比例，右边的是信息


![alt text](/assets/2025-02-05-S6.png)


另外一个输入Ct也是类似上面，不同的是参数矩阵Wc以及激活函数tanh，将数值的范围固定在-1到1之间。
在我的理解里，潜在的长期记忆可以理解成可能可以成为新的长期记忆的部分（现在的短期记忆输入），与遗忘门不一样，字面意思的不一样

## 传送门的更新


![alt text](/assets/2025-02-05-S7.png)


通过对遗忘门，输入们所获得的信息进行Elementwise multiplication操作。
## 输出层


![alt text](/assets/2025-02-05-S8.png)


类似上面的操作一样，也是将上个状态和输入拼接以后乘以参数矩阵Wo，再经过sigmoid函数，输出最终的结果。
最后输出的状态是ot Elementwise multiplication tanh过后的Ct
输出层的主要更新思路就是决定哪些部分遗忘（遗忘门），哪些部分更新（输入门）

![alt text](/assets/2025-02-05-S9.png)


LSTM总共需要四个参数矩阵，这四个参数矩阵的维度都是一样的，都是行是状态的维度，列是行的维度加上输入的维度。
从我自己的理解上来讲，我们可以把tanh看作一种标准化输出的操作，它将输出的范围固定在-1到1之间，这样可以防止梯度消失，它在LSTM中更多表示的是数值（抽象意义上）
而sigmoid函数是一种比例，决定保留多少忘记多少的比例，它在LSTM中更多表示的是比例（抽象意义上）
